{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Modules to be imported \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd \n",
    "import sklearn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import sklearn.datasets\n",
    "import sklearn.linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from itertools import combinations \n",
    "from random import seed\n",
    "from random import randint\n",
    "import random\n",
    "import matplotlib.pyplot as plt \n",
    "from random import randrange\n",
    "from csv import reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ANN \n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def sigmoid_k(x,k):\n",
    "    return 1 / (1 + k*np.exp(-z))\n",
    "\n",
    "def relu(x):\n",
    "    return np.log(1+np.exp(x))\n",
    "\n",
    "def drelu(x):\n",
    "    x[x<=0] = 0\n",
    "    x[x>0] = 1\n",
    "    return x\n",
    "\n",
    "def loss(out,Y):\n",
    "        loss = (-1)*(np.sum(np.multiply(np.log(out), Y) + np.multiply((1 - Y), np.log(1 - out))))/(Y.shape[1])\n",
    "        #print(loss)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ann(X,Y,learning_rate):\n",
    "    #taking all 9 layers as input\n",
    "    in_layer_no=X.shape[0] #no of attributes\n",
    "    hid_layer_no=int(2/3*in_layer_no)\n",
    "    out_layer_no=1\n",
    "\n",
    "    #initial weights\n",
    "    wh = np.random.randn(hid_layer_no,in_layer_no) * 0.01\n",
    "    bh = np.zeros(shape=(hid_layer_no, 1))\n",
    "    w_out = np.random.randn(out_layer_no,hid_layer_no) * 0.01\n",
    "    b_out = np.zeros(shape=(out_layer_no, 1))\n",
    "\n",
    "    initial_weights=[wh,bh,w_out,b_out]\n",
    "    dwh_old=0\n",
    "    dw_out_old=0\n",
    "    for i in range(0,75000):\n",
    "        #forward propogation\n",
    "        #input to hidden layer = dot product(X,wh) + bh\n",
    "        hid_layer_input = np.dot(wh,X) + bh\n",
    "        hid_layer_act = relu(hid_layer_input)\n",
    "        \n",
    "        # Final output layer prediction\n",
    "        out_layer_input = np.dot(w_out,hid_layer_act) + b_out\n",
    "        out_layer_act = sigmoid(out_layer_input)\n",
    "        lo=loss(out_layer_act,Y)\n",
    "        if(i%1000==0):\n",
    "            print(i,lo)\n",
    "        '''dZ2 = out_layer_act - Y\n",
    "        dW2 = (1 /X.shape[1]) * np.dot(dZ2, hid_layer_act.T)\n",
    "        db2 = (1 / X.shape[1]) * np.sum(dZ2, axis=1, keepdims=True)\n",
    "        dZ1 = np.multiply(np.dot(w_out.T, dZ2), 1 - np.power(hid_layer_act, 2))\n",
    "        dW1 = (1 / X.shape[1]) * np.dot(dZ1, X.T)\n",
    "        db1 = (1 / X.shape[1]) * np.sum(dZ1, axis=1, keepdims=True)\n",
    "        \n",
    "        wh = wh - learning_rate * dW1\n",
    "        w_out = w_out - learning_rate * dW2\n",
    "        bh = bh - learning_rate * db1\n",
    "        b_out = b_out - learning_rate * db2\n",
    "        '''\n",
    "        #backward propogation output layer    \n",
    "        t_o = out_layer_act - Y\n",
    "        sigmak = t_o * sigmoid(out_layer_input)*(1-sigmoid(out_layer_input))   \n",
    "        dLoss_W2 = (1/hid_layer_act.shape[1]) * np.dot(sigmak,hid_layer_act.T)\n",
    "        dLoss_b2 = (1/hid_layer_act.shape[1]) * np.dot(sigmak, np.ones([sigmak.shape[1],1])) \n",
    "          \n",
    "        #backward propogation input layer\n",
    "        dLoss_A1 = np.dot(w_out.T,sigmak)\n",
    "        dLoss_Z1 = dLoss_A1 * drelu(hid_layer_input)        \n",
    "        dLoss_A0 = np.dot(wh.T,dLoss_Z1)\n",
    "        dLoss_W1 = 1/X.shape[1] * np.dot(dLoss_Z1,X.T)\n",
    "        dLoss_b1 = 1/X.shape[1] * np.dot(dLoss_Z1, np.ones([dLoss_Z1.shape[1],1]))  \n",
    "        \n",
    "        wh = wh - learning_rate * dLoss_W1\n",
    "        w_out = w_out - learning_rate * dLoss_W2\n",
    "        bh = bh - learning_rate * dLoss_b1\n",
    "        b_out = b_out - learning_rate * dLoss_b2\n",
    "    return [wh,bh,w_out,b_out]\n",
    "    ''' \n",
    "        #backpropogation\n",
    "        dout_layer_act = (out_layer_act - Y) / (out_layer_act * (1 - out_layer_act))\n",
    "        dZ2 = np.multiply(dout_layer_act, out_layer_act * (1 - out_layer_act))\n",
    "        dw_out = np.dot(dZ2, hid_layer_act.T)\n",
    "        #dw_out=dw_out+0.5*dw_out_old\n",
    "        #dw_out_old=dw_out\n",
    "        db_out = np.sum(dZ2, axis=1, keepdims=True)\n",
    "\n",
    "        dhid_layer_act = np.dot(w_out.T, dZ2)\n",
    "        dZ1 = np.multiply(dhid_layer_act, dhid_layer_act * (1 - dhid_layer_act))\n",
    "        dwh = np.dot(dZ1, X.T)\n",
    "        #dwh=dwh+0.2*dwh_old\n",
    "        #dwh_old=dwh\n",
    "        dbh = np.sum(dZ1, axis=1, keepdims=True)\n",
    "        \n",
    "        wh = wh - learning_rate * dwh\n",
    "        w_out = w_out - learning_rate * dw_out\n",
    "        bh = bh - learning_rate * dbh\n",
    "        b_out = b_out - learning_rate * db_out\n",
    "    '''\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#final forward propogation to test trained model \n",
    "def predict1(XTrain,YTrain,XTest,YTest,learning_rate):\n",
    "    final_weights=ann(XTrain,YTrain,learning_rate)\n",
    "    hid_layer_input = np.dot(final_weights[0],XTest) + final_weights[1]\n",
    "    hid_layer_act = relu(hid_layer_input)\n",
    "    out_layer_input = np.dot(final_weights[2],hid_layer_act) + final_weights[3]\n",
    "    out_layer_act = sigmoid(out_layer_input)\n",
    "    predictions = out_layer_act > 0.5\n",
    "    predictions=predictions.astype(int)  \n",
    "    accuracy=float((np.dot(YTest,predictions.T) + np.dot(1-YTest,1-predictions.T))/float(YTest.size)*100)\n",
    "    #print(accuracy)\n",
    "    #print ('Accuracy: %f' % float((np.dot(YTest,predictions.T) + np.dot(1-YTest,1-predictions.T))/float(YTest.size)*100) + '%')\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_test_train(X,Y):\n",
    "    #print(X.shape,Y.shape)\n",
    "    XTrain,XTest,YTrain,YTest=train_test_split(X, Y, test_size=0.20, random_state=42)\n",
    "    X,Y=X.to_numpy(),Y.to_numpy()\n",
    "    XTrain,XTest,YTrain,YTest= XTrain.to_numpy().T,XTest.to_numpy().T,YTrain.to_numpy().reshape(1, YTrain.shape[0]),YTest.to_numpy().reshape(1, YTest.shape[0])\n",
    "    X,Y=X.T,Y.reshape(1, Y.shape[0])\n",
    "    #print(X.shape,Y.shape,XTrain.shape,YTrain.shape,XTest.shape,YTest.shape)\n",
    "    acc=predict1(XTrain,YTrain,XTest,YTest,0.05)\n",
    "    return(acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to create all possible combination of subsets of size r\n",
    "def rSubset(arr,r): \n",
    "    return list(combinations(arr, r)) \n",
    "def comb(r):\n",
    "    arr = [2,3,4,5,6,7,8,9,10] \n",
    "    return(rSubset(arr, r))\n",
    "\n",
    "#function for bagging of attributes \n",
    "def attribute_bagging(r):\n",
    "    s=comb(r)\n",
    "    #print(s,len(s))\n",
    "    length=len(s)\n",
    "    avg=[]\n",
    "    #repeat 5 times\n",
    "    for i in range(0,5):\n",
    "        #9 random values\n",
    "        #generating 9 random values with range given as the 0,number of combination\n",
    "        rand_index=random.sample(range(0, length), 9)\n",
    "        l=[]\n",
    "        for j in rand_index:\n",
    "            t=s[j]\n",
    "            #subsetting the dataframe X based on rand_index values\n",
    "            new_dataset = X.loc[:,list(t)]\n",
    "            #sending the new dataframe to the ann function \n",
    "            acc=split_test_train(new_dataset,Y)\n",
    "            #saving accuracy of each accuracy in a list\n",
    "            l.append(acc)\n",
    "        #finding average of accuracies of each iteration\n",
    "        avg1=sum(l)/len(l) \n",
    "        avg.append(avg1)\n",
    "    return(avg)\n",
    "            \n",
    "    \n",
    "    \n",
    "def ANN_att_bag():\n",
    "    total=[]\n",
    "    #attribute bagging for subset size 2,3,4,5,6,7,8 and 9 each time\n",
    "    for i in range(2,10):\n",
    "        #append accuracies of each subset size\n",
    "        total.append(attribute_bagging(i))\n",
    "    max_avg=[]\n",
    "    #taking max of accuracies for each subset size \n",
    "    for i in total:\n",
    "        max_avg.append(max(i))\n",
    "    #print(max_avg)\n",
    "    #plot to show accuracies for each subset \n",
    "    keys=[2,3,4,5,6,7,8,9]\n",
    "    #converting to a dictionary\n",
    "    dictionary = dict(zip(keys, max_avg))\n",
    "    print(dictionary)\n",
    "    #plotting accuracies for each subset size\n",
    "    plt.plot(*zip(*sorted(dictionary.items())))\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RANDOM FOREST\n",
    "#calculate the optimal split\n",
    "def get_split(dataset, n_features):\n",
    "    class_mid = set(row [-1] for row in dataset)\n",
    "    classes = list(class_mid)\n",
    "    opt_index, opt_value, opt_score, opt_groups = 999, 999, 999, None\n",
    "    feat = list()\n",
    "    while (len(feat) < n_features):\n",
    "        col = randrange(len(dataset[0])-1)\n",
    "        if col not in feat:\n",
    "            feat.append(col)\n",
    "    for col in feat:\n",
    "        for row in dataset:\n",
    "            groups = split_test(col, row[col], dataset)\n",
    "            n_instances = 0 \n",
    "            count = 0\n",
    "            #finding gini index \n",
    "            for group in groups:\n",
    "                n_instances = float(n_instances + len(group))\n",
    "            gini_val_mid = 0.0\n",
    "            for group in groups:\n",
    "                size = int(len(group))\n",
    "                if(size==0):\n",
    "                    continue\n",
    "                score = 0.0\n",
    "                for class_val in classes:\n",
    "                    for row in group:\n",
    "                        if (row[-1] == class_val):\n",
    "                            count = count + 1 \n",
    "                g = count / size \n",
    "                score = score + (g*g)\n",
    "                gini_val_mid = gini_val_mid + (1.0 - score)* (size/ n_instances)\n",
    "            gini_val = gini_val_mid \n",
    "            #gets optimal column to split and the point at which column should be split \n",
    "            if(gini_val < opt_score):\n",
    "                opt_index, opt_value, opt_score, opt_groups = col, row[col], gini_val, groups\n",
    "        return {'index':opt_index, 'value':opt_value, 'groups':opt_groups}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To calculate the groups for the proposed split in data\n",
    "def split_test(index, value, dataset):\n",
    "    left = []\n",
    "    right = []\n",
    "    groups = []\n",
    "    for col in dataset:\n",
    "        if col[index] < value:\n",
    "            left.append(col)\n",
    "        else:\n",
    "            right.append(col)\n",
    "    groups.append(left)\n",
    "    groups.append(right)\n",
    "    return groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to get splits of the data\n",
    "def create_tree(node, min_size, n_features, depth):\n",
    "    left, right = node['groups']\n",
    "    del(node['groups'])\n",
    "    if not left or not right:\n",
    "        #to get the most probable class of the terminal node\n",
    "        outcomes = [row[-1] for row in (left+right)]\n",
    "        node['left'] = node['right'] = max(set(outcomes), key = outcomes.count)\n",
    "        return\n",
    "    if( len(left) <= min_size):\n",
    "        #to get the most probable class of the terminal node\n",
    "        outcomes = [row[-1] for row in left]\n",
    "        node['left'] = max(set(outcomes), key = outcomes.count)\n",
    "    else:\n",
    "        node['left'] = get_split(left, n_features)\n",
    "        create_tree(node['left'], min_size, n_features, depth+1)\n",
    "    if( len(right) <= min_size):\n",
    "        #to get the most probable class of the terminal node\n",
    "        outcomes = [row[-1] for row in (left+right)]\n",
    "        node['right'] = max(set(outcomes), key = outcomes.count)\n",
    "    else:\n",
    "        node['right'] = get_split(right, n_features)\n",
    "        create_tree(node['right'], min_size, n_features, depth+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_forests(train, test, min_size, n_trees, n_features):\n",
    "    forest = list()\n",
    "    predictions =  []\n",
    "    for i in range(n_trees):\n",
    "        tree = get_split(train, n_features)\n",
    "        create_tree(tree, min_size, n_features, 1)\n",
    "        forest.append(tree)\n",
    "    for row in test:\n",
    "        predictions_mid = []\n",
    "        for tree in forest:\n",
    "            prediction = predict(tree,row)\n",
    "            predictions_mid.append(prediction)\n",
    "    #print(predictions)\n",
    "        max_prediction = max(set(predictions_mid), key = predictions_mid.count)\n",
    "        prediction = max_prediction\n",
    "        predictions.append(prediction)\n",
    "    #print(predictions)\n",
    "    return(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(node, row):\n",
    "    #print(node)\n",
    "    #print(node['index'])\n",
    "    #print(row[node['index']])\n",
    "    if(row[int(node['index'])] < node['value']):\n",
    "        if isinstance(node['left'], dict):\n",
    "            return predict(node['left'], row)\n",
    "        else:\n",
    "            return node['left']\n",
    "    else:\n",
    "        if isinstance(node['right'], dict):\n",
    "            return predict(node['right'], row)\n",
    "        else:\n",
    "            return node['right']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_algorithm(dataset,  n_folds, min_size, n_trees, n_features):\n",
    "    #splitting the data into folds \n",
    "    dataset_split = list()\n",
    "    dataset_copy = list(dataset)\n",
    "    fold_size = int(len(dataset) / n_folds)\n",
    "    for j in range(n_folds):\n",
    "        fold = list()\n",
    "        while len(fold) < fold_size:\n",
    "            index = randrange(len(dataset_copy))\n",
    "            fold.append(dataset_copy.pop(index))\n",
    "        dataset_split.append(fold)\n",
    "    folds = dataset_split\n",
    "    scores = list()\n",
    "    #creating test and train datasets\n",
    "    for fold in folds:\n",
    "        train_set = list(folds)\n",
    "        train_set.remove(fold)\n",
    "        train_set = sum(train_set, [])\n",
    "        test_set = list()\n",
    "        for row in fold:\n",
    "            row_copy = list(row)\n",
    "            test_set.append(row_copy)\n",
    "            #row_copy[-1] = 0\n",
    "            row_copy[-1] = None\n",
    "        #getting the predictions for the algorithm \n",
    "        predicted = random_forests(train_set, test_set, min_size, n_trees, n_features)\n",
    "        #print(predicted)\n",
    "        actual = list()\n",
    "        for row in fold:\n",
    "            actual.append(row[-1])\n",
    "        correct = 0\n",
    "    #print(predicted)\n",
    "        for i in range(len(actual)):\n",
    "            if (actual[i] == predicted[i]):\n",
    "                correct += 1\n",
    "        #print(actual)\n",
    "        accuracy = correct / float(len(actual)) * 100.0\n",
    "        scores.append(accuracy)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    #reading preprocessed dataset\n",
    "    df_proc = pd.read_csv(\"preprocessed.csv\") \n",
    "    df_proc.columns=[0,1,2,3,4,5,6,7,8,9,10,11]\n",
    "    #finding X(attributes excluding Target) and Y(Target)\n",
    "    Y = df_proc.filter([11], axis=1)\n",
    "    X = df_proc.drop([0,1,11],axis=1)\n",
    "    #ANN \n",
    "    ANN_att_bag()\n",
    "    #RANDOM FOREST\n",
    "    dataset = []\n",
    "    with open('preprocessed.csv', 'r') as f:\n",
    "        csv_reader = reader(f)\n",
    "        for row in csv_reader:\n",
    "            dataset.append(row)\n",
    "    fold_size = 6\n",
    "    min_termination_size = 1\n",
    "    sum_scores = 0.0\n",
    "    n_features = int((len(dataset[0])-1)/2)\n",
    "    n_trees = 1\n",
    "    score = []\n",
    "    n_trees_list = [1,2,3,4,5,6,7,8,9,10]\n",
    "    mean_accuracies = list()\n",
    "    #print(n_features)\n",
    "    for n_trees in n_trees_list:\n",
    "        scores = evaluation_algorithm(dataset,fold_size, min_termination_size, n_trees, n_features)\n",
    "        print('Trees: %d' % n_trees)\n",
    "        for score in scores:  \n",
    "            sum_scores = sum_scores + score\n",
    "        mean_accuracy = (sum_scores/6)\n",
    "        mean_accuracies.append(mean_accuracy)\n",
    "        sum_scores = 0 \n",
    "        print('Mean Accuracy: %.2f%%' % (mean_accuracy))\n",
    "    plt.plot(n_trees_list, mean_accuracies, color='black', linestyle='dashed', linewidth = 3)\n",
    "    plt.xlabel('Number of trees')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.show()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
